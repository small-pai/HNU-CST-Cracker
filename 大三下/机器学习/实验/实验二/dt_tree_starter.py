import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import classification_report,confusion_matrix,accuracy_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
import matplotlib.pyplot as plt
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder

# Load the datasetåŠ è½½æ•°æ®é›†
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
columns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation',
           'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']
# data = pd.read_csv('./adult/adult.data', names=columns, na_values=' ?')
data = pd.read_csv(url, names=columns, na_values=' ?')
# data overview
# load top 5 rows values from adult datasetå±•ç¤ºæ•°æ®å‰5è¡Œ
data.head(5)
print ("Rows     : " ,data.shape[0])#æ‰“å°æ•°æ®é›†çš„è¡Œæ•°å’Œåˆ—æ•°
print ("Columns  : " ,data.shape[1])
print ("\nFeatures : \n" ,data.columns.tolist())#æ‰“å°æ•°æ®é›†çš„åˆ—å
print ("\nMissing values :  ", data.isnull().sum().values.sum())# æ‰“å°æ•°æ®é›†ä¸­çš„ç¼ºå¤±å€¼æ•°é‡
print ("\nUnique values :  \n",data.nunique())
# Let's understand the type of values present in each column of our adult dataframe 'data'
# The info() method : Used to get a quick description of the data, in particular the total number of rows,columns and each attribute's type and number of non-null values...etc
data.info()
data.describe()

# checking "NaN" total values present in particular 'workclass' featureæ£€æŸ¥ç‰¹å®šâ€œå·¥ä½œç±»â€åŠŸèƒ½ä¸­å­˜åœ¨çš„â€œNaNâ€æ€»å€¼
df_check_missing_workclass = (pd.isnull(data['workclass'])).sum()
df_check_missing_workclass
# checking "NaN" total values present in particular 'occupation' featureæ£€æŸ¥ç‰¹å®šâ€œèŒä¸šâ€ç‰¹å¾ä¸­å­˜åœ¨çš„â€œNaNâ€æ€»å€¼
df_check_missing_occupation = (pd.isnull(data['occupation'])).sum()
df_check_missing_occupation
# checking "NaN" values, how many are there in the whole dataset
df_missing = pd.isnull(data).sum()
df_missing
# find out percentage of "NaN" value present across the dataset
percent_missing = pd.isnull(data).sum() * 100/len(data)
percent_missing
# Let's find total number of rows which doesn't contain any missing value as '?'
data.apply(lambda x: pd.notnull(x),axis=1).sum()

# Drop rows with missing valuesåˆ é™¤åŒ…å«ç¼ºå¤±å€¼çš„è¡Œ
# write your code here -------
data_clean = data.dropna()#ç”¨äºåˆ é™¤æ•°æ®é›†ä¸­åŒ…å«ç¼ºå¤±å€¼çš„è¡Œï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°data_cleanä¸­

# Preprocessing: Convert categorical variables to numerical using one-hot encodingä½¿ç”¨ç‹¬çƒ­ç¼–ç è½¬æ¢åˆ†ç±»å˜é‡ä¸ºæ•°å€¼å‹
data = pd.get_dummies(data, columns=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country'])
# Separate fatures and target variableåˆ†ç¦»ç‰¹å¾å’Œç›®æ ‡å˜é‡
# write your code here -------
X = data.drop('income', axis=1)  # ç‰¹å¾æ•°æ®
y = data['income']               # ç›®æ ‡å˜é‡

# or encode categorical variables using label Encoderæˆ–ä½¿ç”¨æ ‡ç­¾ç¼–ç å™¨ç¼–ç åˆ†ç±»å˜é‡
# select all categorical variablesé€‰æ‹©æ‰€æœ‰åˆ†ç±»å˜é‡
data_categorical = data.select_dtypes(include=['object'])
data_categorical.head()
# Apply label encoder to df_categoricalåº”ç”¨æ ‡ç­¾ç¼–ç å™¨
# write your code here -------
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)  # å¯¹ç›®æ ‡å˜é‡åº”ç”¨æ ‡ç­¾ç¼–ç 
# Next, Concatenate df_categorical dataframe with original df (dataframe
# first, Drop earlier duplicate columns which had categorical valuesä¸¢å¼ƒæ—©å…ˆduplicateçš„åˆ†ç±»åˆ—
# write your code here --------
data.drop(data_categorical.columns, axis=1, inplace=True)#ä»dataæ•°æ®é›†ä¸­åˆ é™¤äº†æ‰€æœ‰çš„åˆ†ç±»æ•°æ®åˆ—ã€‚
#å‚æ•°axis=1è¡¨ç¤ºæŒ‰åˆ—æ“ä½œï¼Œinplace=Trueè¡¨ç¤ºåœ¨åŸæ•°æ®é›†ä¸Šè¿›è¡Œä¿®æ”¹ï¼Œè€Œä¸æ˜¯è¿”å›ä¸€ä¸ªæ–°çš„æ•°æ®é›†ã€‚
data.head()
data.info()

# Next, Since here we have income as target/predicted variable we can see it's showing integer though we need to figure out labelled as <=50ğ¾ ğ‘ğ‘›ğ‘‘ >50K and >50K as categorical.
# Separate features and target variable
# write your code here --------

# Split data into train and test sets using method of train_test_split()å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
# write your code here --------
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
#test_size=0.3ï¼šæŒ‡å®šæµ‹è¯•é›†çš„å¤§å°ä¸º30%ã€‚è¿™æ„å‘³ç€70%çš„æ•°æ®å°†ç”¨äºè®­ç»ƒï¼Œ30%çš„æ•°æ®å°†ç”¨äºæµ‹è¯•ã€‚
#random_state=42ï¼šç”¨æ¥è®¾ç½®éšæœºç§å­ï¼Œè¿™æ ·æ¯æ¬¡è¿è¡Œåˆ’åˆ†æ•°æ®é›†çš„æ“ä½œæ—¶éƒ½èƒ½å¾—åˆ°ç›¸åŒçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œæœ‰åŠ©äºç»“æœçš„å¯é‡ç°æ€§ã€‚
#X_trainï¼šè®­ç»ƒé›†çš„ç‰¹å¾æ•°æ®,X_testï¼šæµ‹è¯•é›†çš„ç‰¹å¾æ•°æ®,y_trainï¼šè®­ç»ƒé›†çš„ç›®æ ‡æ•°æ®,y_testï¼šæµ‹è¯•é›†çš„ç›®æ ‡æ•°æ®
# Build a decision tree classifieråˆ›å»ºå†³ç­–æ ‘åˆ†ç±»å™¨å®ä¾‹
# write your code here --------
dt_classifier = DecisionTreeClassifier()
# Define hyperparameters for tuning
params = {
    'max_depth': [3, 5, 7, 10],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}
# params = {
#     'max_depth': [6, 6.5, 7, 7.5, 8],
#     'min_samples_split': [9, 10, 11],
#     'min_samples_leaf': [3, 4, 5],
# }#æ”¹è¿›çš„å‚æ•°å°è¯•

# Hyperparameter tuning using GridSearchCV
grid_search = GridSearchCV(dt_classifier, params, cv=5)#5æŠ˜äº¤å‰éªŒè¯
grid_search.fit(X_train, y_train)

# Get the best hyperparameters
best_params = grid_search.best_params_
print("Best Hyperparameters:", best_params)

# Train the decision tree classifier with the best parametersä½¿ç”¨æœ€ä½³è¶…å‚æ•°è®­ç»ƒå†³ç­–æ ‘åˆ†ç±»å™¨
best_dt_classifier = DecisionTreeClassifier(**best_params)
# write your code here --------
best_dt_classifier.fit(X_train, y_train)

# Evaluate the model with accuracy
train_accuracy = best_dt_classifier.score(X_train, y_train)
test_accuracy = best_dt_classifier.score(X_test, y_test)
print("Train Accuracy:", train_accuracy)
print("Test Accuracy:", test_accuracy)

# Evaluate the model with more metrics
y_pred = best_dt_classifier.predict(X_test)
print(classification_report(y_test,y_pred))
# Printing confusion matrix and accuracy
print(confusion_matrix(y_test,y_pred))
print(accuracy_score(y_test,y_pred))

# Visualize the decision tree
plt.figure(figsize=(20,10),dpi=600)
plot_tree(best_dt_classifier, feature_names=X.columns, class_names=["<=50K", ">50K"], filled=True)
plt.show()
